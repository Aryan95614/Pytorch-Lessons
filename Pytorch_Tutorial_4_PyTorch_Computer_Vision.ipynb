{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvf1IL8QBQLd4PXdrb9XiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e4dd1a94a2d412f81728067e5aa2e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4def52b3024fc5b03568de39d1bdcc",
              "IPY_MODEL_255e24ece6d34b0a9c2a024ebd948093",
              "IPY_MODEL_482b94d2e01f40e0bf7f9e1280ed4851"
            ],
            "layout": "IPY_MODEL_abaa63105ce4439398ab7f1b2d2f31a8"
          }
        },
        "8f4def52b3024fc5b03568de39d1bdcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a31be05cf0245f5aec7d0fbc57d8940",
            "placeholder": "​",
            "style": "IPY_MODEL_842c096b1932425ba6c8b4fced8ef025",
            "value": "100%"
          }
        },
        "255e24ece6d34b0a9c2a024ebd948093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2979fda1d634113a994fa71af493ce7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a68819628e749b29ea009ea5439288e",
            "value": 3
          }
        },
        "482b94d2e01f40e0bf7f9e1280ed4851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c9b14f82cd744119d40e1f351d743de",
            "placeholder": "​",
            "style": "IPY_MODEL_147215d18f5b4202a69ae96535a0106c",
            "value": " 3/3 [00:25&lt;00:00,  8.64s/it]"
          }
        },
        "abaa63105ce4439398ab7f1b2d2f31a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a31be05cf0245f5aec7d0fbc57d8940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "842c096b1932425ba6c8b4fced8ef025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2979fda1d634113a994fa71af493ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a68819628e749b29ea009ea5439288e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c9b14f82cd744119d40e1f351d743de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "147215d18f5b4202a69ae96535a0106c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35b9317aa0484ff3864208e237da2b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c455b047e523458dbadca80179469062",
              "IPY_MODEL_a8843c87f0214b7e9be5313b907b909c",
              "IPY_MODEL_b9607d7909234c50aca55bc8b2c730a6"
            ],
            "layout": "IPY_MODEL_fb133b1166274f8baa8f0efd69009cbc"
          }
        },
        "c455b047e523458dbadca80179469062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fe5730eb1f940ccb11282e9f8bf7594",
            "placeholder": "​",
            "style": "IPY_MODEL_57e260cf47c44837a90c0c23a4ce25e4",
            "value": "100%"
          }
        },
        "a8843c87f0214b7e9be5313b907b909c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06710d032c734865aecaff27f25fb1e4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06f62e6093524f0e951069bf05970ce9",
            "value": 3
          }
        },
        "b9607d7909234c50aca55bc8b2c730a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e8410a38184b41bdf071b883c3e87e",
            "placeholder": "​",
            "style": "IPY_MODEL_4d9ce65b20c84fb98550ee410a732549",
            "value": " 3/3 [00:25&lt;00:00,  8.59s/it]"
          }
        },
        "fb133b1166274f8baa8f0efd69009cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fe5730eb1f940ccb11282e9f8bf7594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e260cf47c44837a90c0c23a4ce25e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06710d032c734865aecaff27f25fb1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f62e6093524f0e951069bf05970ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2e8410a38184b41bdf071b883c3e87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9ce65b20c84fb98550ee410a732549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b8fde0b98f4e02ab23ec5ee4286c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0ca94c2fb984d48b4ef41911d4b78d8",
              "IPY_MODEL_2eef07416af64ab5b320c9676fc8160d",
              "IPY_MODEL_6858727570884bc498a527117bbe2d95"
            ],
            "layout": "IPY_MODEL_a12f9980f900483e81f33fc6e97f05c7"
          }
        },
        "c0ca94c2fb984d48b4ef41911d4b78d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2616a48e0354c6381f816fc3013c8ab",
            "placeholder": "​",
            "style": "IPY_MODEL_a106e49da79d4820aa02378071a89970",
            "value": "100%"
          }
        },
        "2eef07416af64ab5b320c9676fc8160d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97db9c4322d94386b593ee1a2db6a4f9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3c0020d171045d9925538e0d8893e29",
            "value": 3
          }
        },
        "6858727570884bc498a527117bbe2d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab6aa3ecd80747dcba9873f6f82f9c4e",
            "placeholder": "​",
            "style": "IPY_MODEL_6ebd68fe53d740959ec02bc0f0cb9266",
            "value": " 3/3 [02:51&lt;00:00, 57.64s/it]"
          }
        },
        "a12f9980f900483e81f33fc6e97f05c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2616a48e0354c6381f816fc3013c8ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a106e49da79d4820aa02378071a89970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97db9c4322d94386b593ee1a2db6a4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3c0020d171045d9925538e0d8893e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab6aa3ecd80747dcba9873f6f82f9c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ebd68fe53d740959ec02bc0f0cb9266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan95614/Pytorch-Lessons/blob/RUNTHISFILE/Pytorch_Tutorial_4_PyTorch_Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Where does computer vision get used?\n",
        "If you use a smartphone, you've already used computer vision.\n",
        "\n",
        "Camera and photo apps use computer vision to enhance and sort images.\n",
        "\n",
        "Modern cars use computer vision to avoid other cars and stay within lane lines.\n",
        "\n",
        "Manufacturers use computer vision to identify defects in various products.\n",
        "\n",
        "Security cameras use computer vision to detect potential intruders.\n",
        "\n",
        "In essence, anything that can described in a visual sense can be a potential computer vision problem.\n",
        "\n",
        "This is what we are going to cover:\n",
        "<div class=\"md-typeset__scrollwrap\"><div class=\"md-typeset__table\"><table>\n",
        "<thead><tr>\n",
        "<th><strong>Topic</strong></th>\n",
        "<th><strong>Contents</strong></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td><strong>0. Computer vision libraries in PyTorch</strong></td>\n",
        "<td>PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>1. Load data</strong></td>\n",
        "<td>To practice computer vision, we'll start with some images of different pieces of clothing from <a href=\"https://github.com/zalandoresearch/fashion-mnist\">FashionMNIST</a>.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>2. Prepare data</strong></td>\n",
        "<td>We've got some images, let's load them in with a <a href=\"https://pytorch.org/docs/stable/data.html\">PyTorch <code>DataLoader</code></a> so we can use them with our training loop.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>3. Model 0: Building a baseline model</strong></td>\n",
        "<td>Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong>.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>4. Making predictions and evaluting model 0</strong></td>\n",
        "<td>Let's make some predictions with our baseline model and evaluate them.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>5. Setup device agnostic code for future models</strong></td>\n",
        "<td>It's best practice to write device-agnostic code, so let's set it up.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>6. Model 1: Adding non-linearity</strong></td>\n",
        "<td>Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>7. Model 2: Convolutional Neural Network (CNN)</strong></td>\n",
        "<td>Time to get computer vision specific and introduce the powerful convolutional neural network architecture.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>8. Comparing our models</strong></td>\n",
        "<td>We've built three different models, let's compare them.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>9. Evaluating our best model</strong></td>\n",
        "<td>Let's make some predictons on random images and evaluate our best model.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>10. Making a confusion matrix</strong></td>\n",
        "<td>A confusion matrix is a great way to evaluate a classification model, let's see how we can make one.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>11. Saving and loading the best performing model</strong></td>\n",
        "<td>Since we might want to use our model for later, let's save it and make sure it loads back in correctly.</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table></div></div>"
      ],
      "metadata": {
        "id": "MWv-0aEp83f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Computer vision libraries in PyTorch\n",
        "\n",
        "Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.\n",
        "\n",
        "<div class=\"md-typeset__scrollwrap\"><div class=\"md-typeset__table\"><table>\n",
        "<thead><tr>\n",
        "<th>PyTorch module</th>\n",
        "<th>What does it do?</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/vision/stable/index.html\"><code>torchvision</code></a></td>\n",
        "<td>Contains datasets, model architectures and image transformations often used for computer vision problems.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/vision/stable/datasets.html\"><code>torchvision.datasets</code></a></td>\n",
        "<td>Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains <a href=\"https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets\">a series of base classes for making custom datasets</a>.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/vision/stable/models.html\"><code>torchvision.models</code></a></td>\n",
        "<td>This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/vision/stable/transforms.html\"><code>torchvision.transforms</code></a></td>\n",
        "<td>Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\"><code>torch.utils.data.Dataset</code></a></td>\n",
        "<td>Base dataset class for PyTorch.</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><a href=\"https://pytorch.org/docs/stable/data.html#module-torch.utils.data\"><code>torch.utils.data.DataLoader</code></a></td>\n",
        "<td>Creates a Python iteralbe over a dataset (created with <code>torch.utils.data.Dataset</code>).</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table></div></div>\n",
        "\n",
        "Note: The torch.utils.data.Dataset and torch.utils.data.DataLoader classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.\n",
        "\n",
        "Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies."
      ],
      "metadata": {
        "id": "UKyVmX9m9xE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBPN1n678nm7",
        "outputId": "333d10ae-b6db-403f-f0cf-bdc4a996d57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 1.13.0+cu116\n",
            "torchvision version: 0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision \n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check versions\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision.datasets contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.\n",
        "\n",
        "Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.\n",
        "\n",
        "PyTorch has a bunch of common computer vision datasets stored in torchvision.datasets.\n",
        "\n",
        "Including FashionMNIST in torchvision.datasets.FashionMNIST().\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "\n",
        "- root: str - which folder do you want to download the data to?\n",
        "- train: Bool - do you want the training or test split?\n",
        "- download: Bool - should the data be downloaded?\n",
        "- transform: torchvision.transforms - what transformations would you like to do on the data?\n",
        "- target_transform - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Many other datasets in torchvision have these parameter options.\n",
        "\n",
        "##1. Getting a dataset\n",
        "\n",
        "To begin working on a computer vision problem, let's get a computer vision dataset.\n",
        "\n",
        "We're going to start with FashionMNIST.\n",
        "\n",
        "MNIST stands for Modified National Institute of Standards and Technology.\n",
        "\n",
        "The original MNIST dataset contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\n",
        "\n",
        "FashionMNIST, made by Zalando Research, is a similar setup.\n",
        "\n",
        "Except it contains grayscale images of 10 different kinds of clothing."
      ],
      "metadata": {
        "id": "407gC_m3-y8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to?\n",
        "    train=True, # get training data\n",
        "    download=True, # download data if it doesn't exist on disk\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
        "    target_transform=None # you can transform labels as well\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "McPJhMlUAzhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img  src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-input-and-output-shapes.png\" />\n",
        "\n",
        "If color_channels=3, the image comes in pixel values for red, green and blue (this is also known a the RGB color model).\n",
        "\n",
        "The order of our current tensor is often referred to as CHW (Color Channels, Height, Width).\n",
        "\n",
        "There's debate on whether images should be represented as CHW (color channels first) or HWC (color channels last).\n",
        "\n",
        "PyTorch generally accepts NCHW (channels first) as the default for many operators.\n",
        "\n",
        "However, PyTorch also explains that NHWC (channels last) performs better and is considered best practice.\n",
        "\n",
        "For now, since our dataset and models are relatively small, this won't make too much of a difference.\n",
        "\n",
        "But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).\n",
        "\n",
        "Let's check out more shapes of our data."
      ],
      "metadata": {
        "id": "WkXwe7QqBEOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples are there? \n",
        "print(len(train_data.data))\n",
        "print(len(train_data.targets))\n",
        "print(len(test_data.data))\n",
        "print(len(test_data.targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z42B750PpXpr",
        "outputId": "058bd309-0b28-492a-fa96-c955c8b79f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "60000\n",
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See classes\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ySZ4GeqnDc",
        "outputId": "1f798532-4f95-4b0c-c6e9-2d01e2740cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweet! It looks like we're dealing with 10 different kinds of clothes.\n",
        "\n",
        "Because we're working with 10 different classes, it means our problem is multi-class classification.\n",
        "\n",
        "Let's get visual.\n",
        "\n",
        "###1.2 Visualizing our data"
      ],
      "metadata": {
        "id": "eH5KS0rirQ3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image, label = train_data[0] # first example\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
        "plt.title(label);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "V0fnEHrmrgKR",
        "outputId": "c2191214-3650-4cf0-c555-2b7e70776cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can turn the image into grayscale using the cmap parameter of plt.imshow()."
      ],
      "metadata": {
        "id": "UIPjKM_3r_Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "OBuyFlXYr-qv",
        "outputId": "572ecd77-0ebc-4aa8-b1a2-808572c0e874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAU80lEQVR4nO3de6zcZZ3H8ffHcu3FQiktbamUS1lxDZa1FlYK4SII/AEoWCUbLRGsIZrVjSayahY2rgvBeyJxU4EFF8U1ESKGm0jWoCm3A6ltoagttNLbKdACbSn0wnf/mF/xWM/veQ4zc2amfT6vZHJm5nuemWd+c77n95v5/p7nUURgZnu/t3W7A2bWGU52s0I42c0K4WQ3K4ST3awQTnazQjjZ93KSQtIxbzWWecxLJf2u9d5ZJznZ9xCSfiNpo6T9u92X4SLpNEmrut2PvZWTfQ8gaRpwChDA+V3tjO2xnOx7hk8ADwM3A3MHBiTdLOl6SXdJ2iTpEUlHD/YgkmZLek7SaYPE9pf0TUl/ltQv6b8kHZjokyR9X9LLkp6WdOaAwGRJd0raIGmZpE/t9jzflbSmuny3um8UcA8wWdLm6jL5rWwkS3Oy7xk+Afy4unxQ0sTd4h8D/h04GFgGfH33B5B0DnAbcFFE/GaQ57gWOBaYARwDTAH+LdGnE4HlwHjgKuB2SeOq2E+BVcBk4GLgPyWdUcW+ApxUPc97gFnAVyNiC3AusCYiRleXNYnnt7cqInzp4QswG9gOjK9uPw38y4D4zcANA26fBzw94HYA/wqsBN6922MHjcQWsAU4ekDsH4Fna/p0KbAG0ID7HgU+DkwFdgJjBsSuAW6uri8HzhsQ+yCworp+GrCq29t8b714z9775gK/iogXqts/YbdDeWDdgOuvAqN3i38e+FlELKl5jkOBkcDjkl6S9BJwb3V/ndVRZWhlJY09+WRgQ0Rs2i02pbo+ubq9ezsbZvt0uwNWr/rMPAcYIWlXQu8PHCTpPRHx+yE+1EeAGyWtiojvDRJ/AdgK/H1ErB7iY06RpAEJ/w7gThp7/HGSxgxI+HcAux53DXAE8OSA2K7DdQ/BHEbes/e2C2kcEr+LxmfcGcBxwG9pfI4fqjXAmcDnJF2xezAi3gB+CHxH0gQASVMkfTDxmBOAf5a0r6SPVP26OyKeAxYA10g6QNLxwGXArVW724CvSjpU0nga3wvsivUDh0ga+xZemw2Rk723zQX+OyL+HBHrdl2A7wP/JGnIR2YR8WcaCX+lpMsH+ZUv0fhy72FJrwC/Bv4u8ZCPANNpHBV8Hbg4Il6sYpcA02j8k7kDuCoifl3F/gPoAxYBi4EnqvuIiKdp/DN4pvo44cP7NtJff+wys72V9+xmhXCymxXCyW5WCCe7WSE6WmeX5G8DzYZZRGiw+1vas0s6R9IfqsEOV7byWGY2vJouvUkaAfwROIvGoIfHgEsi4qlEG+/ZzYbZcOzZZwHLIuKZiNhGY6TTBS08npkNo1aSfQrw3IDbq/jLYIc3SZonqU9SXwvPZWYtGvYv6CJiPjAffBhv1k2t7NlX0xi7vMvh/GVkk5n1mFaS/TFguqQjJe1HY7aUO9vTLTNrt6YP4yNih6TPAvcBI4CbIuLJTDMz65KOjnrzZ3az4TcsJ9WY2Z7DyW5WCCe7WSGc7GaFcLKbFcLJblYIJ7tZIZzsZoVwspsVwsluVggnu1khnOxmhXCymxXCSzbv5aRBB0C9qdVRj2PGjEnGZ8+eXRu75557Wnru3GsbMWJEbWzHjh0tPXercn1PafY9857drBBOdrNCONnNCuFkNyuEk92sEE52s0I42c0K4Tr7Xu5tb0v/P9+5c2cyfswxxyTjl19+eTK+devW2tiWLVuSbV977bVk/NFHH03GW6ml5+rgue2aa99K31LnD6TeT+/ZzQrhZDcrhJPdrBBOdrNCONnNCuFkNyuEk92sEK6z7+VSNVnI19nPOOOMZPwDH/hAMr5q1ara2P77759sO3LkyGT8rLPOSsZvuOGG2lh/f3+ybW7MeG675YwePbo29sYbbyTbvvrqq009Z0vJLmkFsAnYCeyIiJmtPJ6ZDZ927NlPj4gX2vA4ZjaM/JndrBCtJnsAv5L0uKR5g/2CpHmS+iT1tfhcZtaCVg/jZ0fEakkTgPslPR0RDw78hYiYD8wHkNTa7IZm1rSW9uwRsbr6uR64A5jVjk6ZWfs1neySRkkas+s6cDawpF0dM7P2auUwfiJwRzVudx/gJxFxb1t6ZW2zbdu2ltq/733vS8anTZuWjKfq/Lkx4ffdd18yfsIJJyTj1113XW2sry/9FdLixYuT8aVLlybjs2alD3JT23XBggXJtg899FBtbPPmzbWxppM9Ip4B3tNsezPrLJfezArhZDcrhJPdrBBOdrNCONnNCqFWl+x9S0/mM+iGRWra4tz7mxsmmipfARx00EHJ+Pbt22tjuaGcOY899lgyvmzZstpYqyXJSZMmJeOp1w3pvl988cXJttdff31trK+vj1deeWXQPwjv2c0K4WQ3K4ST3awQTnazQjjZzQrhZDcrhJPdrBCus/eA3PK+rci9vw8//HAynhvCmpN6bblli1uthaeWfM7V+J944olkPFXDh/xrO+ecc2pjRx11VLLtlClTkvGIcJ3drGROdrNCONnNCuFkNyuEk92sEE52s0I42c0K4SWbe0Anz3XY3caNG5Px3LjtrVu3JuOpZZn32Sf955da1hjSdXSAAw88sDaWq7Ofcsopyfj73//+ZDw3TfaECRNqY/feOzwzsnvPblYIJ7tZIZzsZoVwspsVwsluVggnu1khnOxmhXCdvXAjR45MxnP14lz81VdfrY29/PLLybYvvvhiMp4ba586fyE3h0DudeW2286dO5PxVJ1/6tSpybbNyu7ZJd0kab2kJQPuGyfpfkl/qn4ePCy9M7O2Gcph/M3A7tNqXAk8EBHTgQeq22bWw7LJHhEPAht2u/sC4Jbq+i3AhW3ul5m1WbOf2SdGxNrq+jpgYt0vSpoHzGvyecysTVr+gi4iIjWRZETMB+aDJ5w066ZmS2/9kiYBVD/Xt69LZjYcmk32O4G51fW5wC/a0x0zGy7Zw3hJtwGnAeMlrQKuAq4FfibpMmAlMGc4O7m3a7Xmm6rp5saET548ORl//fXXW4qnxrPn5oVP1eghvzZ8qk6fq5Pvt99+yfimTZuS8bFjxybjixYtqo3l3rOZM2fWxp566qnaWDbZI+KSmtCZubZm1jt8uqxZIZzsZoVwspsVwsluVggnu1khPMS1B+Smkh4xYkQyniq9ffSjH022Peyww5Lx559/PhlPTdcM6aGco0aNSrbNDfXMle5SZb/t27cn2+amuc697kMOOSQZv/7662tjM2bMSLZN9S1VxvWe3awQTnazQjjZzQrhZDcrhJPdrBBOdrNCONnNCqFOLhfsmWoGl6vp7tixo+nHPvHEE5Pxu+66KxnPLcncyjkAY8aMSbbNLcmcm2p63333bSoG+XMAcktd56Re2ze+8Y1k21tvvTUZj4hBi+3es5sVwsluVggnu1khnOxmhXCymxXCyW5WCCe7WSH2qPHsqbG6uXpvbjrm3HTOqfHPqTHbQ9FKHT3n7rvvTsa3bNmSjOfq7Lkpl1PnceTGyufe0wMOOCAZz41Zb6Vt7j3P9f3444+vjeWWsm6W9+xmhXCymxXCyW5WCCe7WSGc7GaFcLKbFcLJblaInqqztzI2ejhr1cPt1FNPTcYvuuiiZPzkk0+ujeWWPc6NCc/V0XNj8VPvWa5vub+H1LzwkK7D5+ZxyPUtJ7fdNm/eXBv78Ic/nGz7y1/+sqk+Zffskm6StF7SkgH3XS1ptaSF1eW8pp7dzDpmKIfxNwPnDHL/dyJiRnVJn6ZlZl2XTfaIeBDY0IG+mNkwauULus9KWlQd5h9c90uS5knqk9TXwnOZWYuaTfYfAEcDM4C1wLfqfjEi5kfEzIiY2eRzmVkbNJXsEdEfETsj4g3gh8Cs9nbLzNqtqWSXNGnAzQ8BS+p+18x6Q3beeEm3AacB44F+4Krq9gwggBXApyNibfbJujhv/Lhx45LxyZMnJ+PTp09vum2ubnrssccm46+//noynhqrnxuXnVtnfM2aNcl4bv71VL05t4Z5bv31kSNHJuMLFiyojY0ePTrZNnfuQ248e25Memq79ff3J9sed9xxyXjdvPHZk2oi4pJB7r4x187MeotPlzUrhJPdrBBOdrNCONnNCuFkNytETy3ZfNJJJyXbf+1rX6uNHXroocm2Bx10UDKeGooJ6eGWL730UrJtbvhtroSUK0GlpsHOTQW9dOnSZHzOnDnJeF9f+izo1LLMBx9ce5Y1ANOmTUvGc5555pnaWG656E2bNiXjuSGwuZJmqvT39re/Pdk29/fiJZvNCudkNyuEk92sEE52s0I42c0K4WQ3K4ST3awQHa+zp+rVDz30ULL9pEmTamO5Onku3srUwbkpj3O17laNHTu2NjZ+/Phk20svvTQZP/vss5PxK664IhlPDZF97bXXkm2fffbZZDxVR4f0sORWh9fmhvbm6vip9rnhs0cccUQy7jq7WeGc7GaFcLKbFcLJblYIJ7tZIZzsZoVwspsVoqN19vHjx8f5559fG7/22muT7ZcvX14by00NnIvnlv9NydVcU3VwgOeeey4Zz03nnBrLn5pmGuCwww5Lxi+88MJkPLUsMqTHpOfek/e+970txVOvPVdHz2233JLMOak5CHJ/T6l5H9atW8e2bdtcZzcrmZPdrBBOdrNCONnNCuFkNyuEk92sEE52s0JkV3GVNBX4ETCRxhLN8yPie5LGAf8LTKOxbPOciNiYeqwdO3awfv362niu3pwaI5xb1jj32Lmab6qumpvne8OGDcn4ypUrk/Fc31Lj5XNjxnNz2t9xxx3J+OLFi5PxVJ09t4x2rhaem68/tVx17nXnxpTnauG59qk6e66Gn1riO7VNhrJn3wF8ISLeBZwEfEbSu4ArgQciYjrwQHXbzHpUNtkjYm1EPFFd3wQsBaYAFwC3VL92C5A+1crMuuotfWaXNA04AXgEmBgRa6vQOhqH+WbWo4ac7JJGAz8HPh8RrwyMReME+0FPspc0T1KfpL7cZzAzGz5DSnZJ+9JI9B9HxO3V3f2SJlXxScCg37xFxPyImBkRM1sdPGBmzcsmuxpfG94ILI2Ibw8I3QnMra7PBX7R/u6ZWbtkS2/AycDHgcWSFlb3fRm4FviZpMuAlUB6bV8apZTVq1fXxnPDbVetWlUbGzVqVLJtbkrlXBnnhRdeqI09//zzybb77JPezLnhtbkyT2qYaW5K49xQztTrBjjuuOOS8S1bttTGcuXQjRuTldzsdkv1PVWWg3xpLtc+t2Rzamjxyy+/nGw7Y8aM2tiSJUtqY9lkj4jfAXVFwTNz7c2sN/gMOrNCONnNCuFkNyuEk92sEE52s0I42c0KMZQ6e9ts3bqVhQsX1sZvv/322hjAJz/5ydpYbrrl3PK+uaGgqWGmuTp4ruaaO7MwtyR0anhvbqnq3LkNuaWs165dm4ynHj/Xt9z5Ca28Z60On21leC2k6/hHHnlksm1/f39Tz+s9u1khnOxmhXCymxXCyW5WCCe7WSGc7GaFcLKbFaKjSzZLaunJzj333NrYF7/4xWTbCRMmJOO5cdupumquXpyrk+fq7Ll6c+rxU1MWQ77OnjuHIBdPvbZc21zfc1LtU7Xqoci9Z7mppFPj2RctWpRsO2dOeuqIiPCSzWYlc7KbFcLJblYIJ7tZIZzsZoVwspsVwsluVoiO19lT85TnapOtOP3005Pxa665JhlP1enHjh2bbJubmz1Xh8/V2XN1/pTUEtqQr8On1gGA9Hu6efPmZNvcdslJ9T033jw3jj/3nt5///3J+NKlS2tjCxYsSLbNcZ3drHBOdrNCONnNCuFkNyuEk92sEE52s0I42c0Kka2zS5oK/AiYCAQwPyK+J+lq4FPArsXJvxwRd2ceq3NF/Q565zvfmYy3ujb84YcfnoyvWLGiNparJy9fvjwZtz1PXZ19KItE7AC+EBFPSBoDPC5p1xkD34mIb7ark2Y2fLLJHhFrgbXV9U2SlgJThrtjZtZeb+kzu6RpwAnAI9Vdn5W0SNJNkg6uaTNPUp+kvpZ6amYtGXKySxoN/Bz4fES8AvwAOBqYQWPP/63B2kXE/IiYGREz29BfM2vSkJJd0r40Ev3HEXE7QET0R8TOiHgD+CEwa/i6aWatyia7GlN03ggsjYhvD7h/0oBf+xCwpP3dM7N2GUrpbTbwW2AxsGu84peBS2gcwgewAvh09WVe6rH2ytKbWS+pK73tUfPGm1mex7ObFc7JblYIJ7tZIZzsZoVwspsVwsluVggnu1khnOxmhXCymxXCyW5WCCe7WSGc7GaFcLKbFcLJblaIocwu204vACsH3B5f3deLerVvvdovcN+a1c6+HVEX6Oh49r95cqmvV+em69W+9Wq/wH1rVqf65sN4s0I42c0K0e1kn9/l50/p1b71ar/AfWtWR/rW1c/sZtY53d6zm1mHONnNCtGVZJd0jqQ/SFom6cpu9KGOpBWSFkta2O316ao19NZLWjLgvnGS7pf0p+rnoGvsdalvV0taXW27hZLO61Lfpkr6P0lPSXpS0ueq+7u67RL96sh26/hndkkjgD8CZwGrgMeASyLiqY52pIakFcDMiOj6CRiSTgU2Az+KiHdX910HbIiIa6t/lAdHxJd6pG9XA5u7vYx3tVrRpIHLjAMXApfSxW2X6NccOrDdurFnnwUsi4hnImIb8FPggi70o+dFxIPAht3uvgC4pbp+C40/lo6r6VtPiIi1EfFEdX0TsGuZ8a5uu0S/OqIbyT4FeG7A7VX01nrvAfxK0uOS5nW7M4OYOGCZrXXAxG52ZhDZZbw7abdlxntm2zWz/Hmr/AXd35odEf8AnAt8pjpc7UnR+AzWS7XTIS3j3SmDLDP+pm5uu2aXP29VN5J9NTB1wO3Dq/t6QkSsrn6uB+6g95ai7t+1gm71c32X+/OmXlrGe7BlxumBbdfN5c+7keyPAdMlHSlpP+BjwJ1d6MffkDSq+uIESaOAs+m9pajvBOZW1+cCv+hiX/5KryzjXbfMOF3edl1f/jwiOn4BzqPxjfxy4Cvd6ENNv44Cfl9dnux234DbaBzWbafx3cZlwCHAA8CfgF8D43qob/9DY2nvRTQSa1KX+jabxiH6ImBhdTmv29su0a+ObDefLmtWCH9BZ1YIJ7tZIZzsZoVwspsVwsluVggnu1khnOxmhfh/X2+bv81H5g8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm, this dataset doesn't look too aesthetic.\n",
        "\n",
        "But the principles we're going to learn on how to build a model for it will be similar across a wide range of computer vision problems.\n",
        "\n",
        "In essence, taking pixel values and building a model to find patterns in them to use on future pixel values.\n",
        "\n",
        "Plus, even for this small dataset (yes, even 60,000 images in deep learning is considered quite small), could you write a program to classify each one of them?\n",
        "\n",
        "You probably could.\n",
        "\n",
        "But I think coding a model in PyTorch would be faster.\n",
        "\n",
        "##2. Prepare DataLoader\n",
        "\n",
        "Now we've got a dataset ready to go.\n",
        "\n",
        "The next step is to prepare it with a torch.utils.data.DataLoader or DataLoader for short.\n",
        "\n",
        "The DataLoader does what you think it might do.\n",
        "\n",
        "It helps load data into a model.\n",
        "\n",
        "For training and for inference.\n",
        "\n",
        "It turns a large Dataset into a Python iterable of smaller chunks.\n",
        "\n",
        "These smaller chunks are called batches or mini-batches and can be set by the batch_size parameter.\n",
        "\n",
        "Why do this?\n",
        "\n",
        "Because it's more computationally efficient.\n",
        "\n",
        "In an ideal world you could do the forward pass and backward pass across all of your data at once.\n",
        "\n",
        "But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.\n",
        "\n",
        "It also gives your model more opportunities to improve.\n",
        "\n",
        "With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
        "\n",
        "What's a good batch size?\n",
        "\n",
        "32 is a good place to start for a fair amount of problems.\n",
        "\n",
        "But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).\n",
        "\n",
        "<img src= \"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HmGIhYnysz-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
        "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
        "    shuffle=True # shuffle data every epoch?\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False # don't necessarily have to shuffle the testing data\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pWaKCyIvWM5",
        "outputId": "0f4a4c09-f445-4735-9efd-b495d51fad2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7f1308453340>, <torch.utils.data.dataloader.DataLoader object at 0x7f1308453580>)\n",
            "Length of train dataloader: 1875 batches of 32\n",
            "Length of test dataloader: 313 batches of 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside the training dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "print(train_features_batch.shape)\n",
        "print(train_labels_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ukjg8XvpRP",
        "outputId": "90bca088-85df-4ee3-b3da-964f979637c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"Off\");\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ulur-TJrv9nm",
        "outputId": "db4670dd-f5dd-46af-fdd5-e739caa37db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: torch.Size([1, 28, 28])\n",
            "Label: 9, label size: torch.Size([])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Model 0: Build a baseline model\n",
        "\n",
        "Data loaded and prepared!\n",
        "\n",
        "Time to build a baseline model by subclassing nn.Module.\n",
        "\n",
        "A baseline model is one of the simplest models you can imagine.\n",
        "\n",
        "You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\n",
        "\n",
        "Our baseline will consist of two nn.Linear() layers.\n",
        "\n",
        "We've done this in a previous section but there's going to one slight difference.\n",
        "\n",
        "Because we're working with image data, we're going to use a different layer to start things off.\n",
        "\n",
        "And that's the nn.Flatten() layer.\n",
        "\n",
        "nn.Flatten() compresses the dimensions of a tensor into a single vector.\n",
        "\n",
        "This is easier to understand when you see it."
      ],
      "metadata": {
        "id": "etW7E6d5wWBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52SqLXNwyq2",
        "outputId": "129e51cc-4225-48c5-822a-12ce98e973c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FirstModel(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # neural networks like their inputs in vector form\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "gDfGeOOtxg7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful!\n",
        "\n",
        "We've got a baseline model class we can use, now let's instantiate a model.\n",
        "\n",
        "We'll need to set the following parameters:\n",
        "\n",
        "- input_shape=784 - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).\n",
        "- hidden_units=10 - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we'll start with 10.\n",
        "- output_shape=len(class_names) - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.\n",
        "\n",
        "Let's create an instance of our model and send to the CPU for now (we'll run a small test for running model_0 on CPU vs. a similar model on GPU soon)."
      ],
      "metadata": {
        "id": "Qm-pbG7_xssq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Need to setup model with input parameters\n",
        "model_0 = FirstModel(input_shape=784, # one for every pixel (28x28)\n",
        "    hidden_units=10, # how many units in the hiden layer\n",
        "    output_shape=len(class_names) # one for every class\n",
        ")\n",
        "model_0.to(\"cpu\") # keep model on CPU to begin with "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njZvZN3fxzo9",
        "outputId": "0504c7f4-7090-4d06-de34-888a3581891e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FirstModel(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "Since we're working on a classification problem, let's bring in our helper_functions.py script and subsequently the accuracy_fn() we defined in notebook 02.\n",
        "\n",
        "Note: Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the TorchMetrics package."
      ],
      "metadata": {
        "id": "hle7fpM4yiId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path \n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy()\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJgPPyPvywQs",
        "outputId": "db739f64-b7ee-4fac-e085-ed1fcb4240a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists, skipping download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 Creating a function to time our experiments\n",
        "\n",
        "Loss function and optimizer ready!\n",
        "\n",
        "It's time to start training a model.\n",
        "\n",
        "But how about we do a little experiment while we train.\n",
        "\n",
        "I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
        "\n",
        "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
        "\n",
        "Our timing function will import the timeit.default_timer() function from the Python timeit module."
      ],
      "metadata": {
        "id": "9kXMW06iz09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer \n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "    \n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "XAkfku2Kz51z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.3 Creating a training loop and training a model on batches of data\n",
        "Beautiful!\n",
        "\n",
        "Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.\n",
        "\n",
        "Let's now create a training loop and a testing loop to train and evaluate our model.\n",
        "\n",
        "We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.\n",
        "\n",
        "Our data batches are contained within our DataLoaders, train_dataloader and test_dataloader for the training and test data splits respectively.\n",
        "\n",
        "A batch is BATCH_SIZE samples of X (features) and y (labels), since we're using BATCH_SIZE=32, our batches have 32 samples of images and targets.\n",
        "\n",
        "And since we're computing on batches of data, our loss and evaluation metrics will be calculated per batch rather than across the whole dataset.\n",
        "\n",
        "This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
        "\n",
        "Let's step through it:\n",
        "\n",
        "Loop through epochs.\n",
        "Loop through training batches, perform training steps, calculate the train loss per batch.\n",
        "Loop through testing batches, perform testing steps, calculate the test loss per batch.\n",
        "Print out what's happening.\n",
        "Time it all (for fun).\n",
        "A fair few steps but...\n",
        "\n",
        "...if in doubt, code it out."
      ],
      "metadata": {
        "id": "To6D7T5_0jjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "pqYNbPL4IeWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        model_0.train() \n",
        "        # 1. Forward pass\n",
        "        y_pred = model_0(X)\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch \n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        if batch % 400 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "    \n",
        "    ### Testing\n",
        "    \n",
        "    test_loss, test_acc = 0, 0 \n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = model_0(X)\n",
        "           \n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "        \n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time      \n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model_0.parameters()).device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "1e4dd1a94a2d412f81728067e5aa2e84",
            "8f4def52b3024fc5b03568de39d1bdcc",
            "255e24ece6d34b0a9c2a024ebd948093",
            "482b94d2e01f40e0bf7f9e1280ed4851",
            "abaa63105ce4439398ab7f1b2d2f31a8",
            "6a31be05cf0245f5aec7d0fbc57d8940",
            "842c096b1932425ba6c8b4fced8ef025",
            "b2979fda1d634113a994fa71af493ce7",
            "2a68819628e749b29ea009ea5439288e",
            "2c9b14f82cd744119d40e1f351d743de",
            "147215d18f5b4202a69ae96535a0106c"
          ]
        },
        "id": "Zs7z0Yzo1CCK",
        "outputId": "4f7a6180-7783-4c47-f1fb-713b4a010315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e4dd1a94a2d412f81728067e5aa2e84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n",
            "\n",
            "Epoch: 1\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n",
            "\n",
            "Epoch: 2\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "\n",
            "Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n",
            "\n",
            "Train time on cpu: 26.000 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Make predictions and get Model 0 results\n",
        "\n",
        "Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.\n",
        "\n",
        "Namely, let's create a function that takes in a trained model, a DataLoader, a loss function and an accuracy function.\n",
        "\n",
        "The function will use the model to make predictions on the data in the DataLoader and then we can evaluate those predictions using the loss function and accuracy function."
      ],
      "metadata": {
        "id": "d15s_0az2BqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module, \n",
        "               data_loader: torch.utils.data.DataLoader, \n",
        "               loss_fn: torch.nn.Module, \n",
        "               accuracy_fn):\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Make predictions with the model\n",
        "            y_pred = model(X)\n",
        "            \n",
        "            # Accumulate the loss and accuracy values per batch\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, \n",
        "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
        "        \n",
        "        # Scale loss and acc to find the average loss/acc per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "        \n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
        ")"
      ],
      "metadata": {
        "id": "VwbKDZPY2ZCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##5. Model 1: Building a better model with non-linearity\n",
        "\n",
        "We learned about the power of non-linearity in notebook 02.\n",
        "\n",
        "Seeing the data we've been working with, do you think it needs non-linear functions?\n",
        "\n",
        "And remember, linear means straight and non-linear means non-straight.\n",
        "\n",
        "Let's find out.\n",
        "\n",
        "We'll do so by recreating a similar model to before, except this time we'll put non-linear functions (nn.ReLU()) in between each linear layer."
      ],
      "metadata": {
        "id": "iTgLPo0Q24H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "2XL52OBM2_6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks good.\n",
        "\n",
        "Now let's instantiate it with the same settings we used before.\n",
        "\n",
        "We'll need input_shape=784 (equal to the number of features of our image data), hidden_units=10 (starting small and the same as our baseline model) and output_shape=len(class_names) (one output unit per class).\n",
        "\n",
        "Note: Notice how we kept most of the settings of our model the same except for one change: adding non-linear layers. This is a standard practice for running a series of machine learning experiments, change one thing and see what happens, then do it again, again, again."
      ],
      "metadata": {
        "id": "KHrXFUA53QXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = Model2(input_shape=784, # number of input features\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names) # number of output classes desired\n",
        ")"
      ],
      "metadata": {
        "id": "9wMu167D1K7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "As usual, we'll setup a loss function, an optimizer and an evaluation metric (we could do multiple evaluation metrics but we'll stick with accuracy for now)."
      ],
      "metadata": {
        "id": "oVS24eHB2SDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # for multi-classification\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), \n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "CZrzGULp2kVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2 Functionizing training and test loops\n",
        "\n",
        "So far we've been writing train and test loops over and over.\n",
        "\n",
        "Let's write them again but this time we'll put them in functions so they can be called again and again.\n",
        "\n",
        "And because we're using device-agnostic code now, we'll be sure to call .to(device) on our feature (X) and target (y) tensors.\n",
        "\n",
        "For the training loop we'll create a function called train_step() which takes in a model, a DataLoader a loss function and an optimizer.\n",
        "\n",
        "The testing loop will be similar but it'll be called test_step() and it'll take in a model, a DataLoader, a loss function and an evaluation function."
      ],
      "metadata": {
        "id": "l1Dd3oWr32Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -> pred labels\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode(): \n",
        "        for X, y in data_loader:\n",
        "            # Send data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)\n",
        "            \n",
        "            # 2. Calculate loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) # Go from logits -> pred labels\n",
        "            )\n",
        "        \n",
        "        # Adjust metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "CdgMJrF34b88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Woohoo!\n",
        "\n",
        "Now we've got some functions for training and testing our model, let's run them.\n",
        "\n",
        "We'll do so inside another loop for each epoch.\n",
        "\n",
        "That way for each epoch we're going a training and a testing step.\n",
        "\n",
        "Note: You can customize how often you do a testing step. Sometimes people do them every five epochs or 10 epochs or in our case, every epoch.\n",
        "\n",
        "Let's also time things to see how long our code takes to run on the GPU."
      ],
      "metadata": {
        "id": "8Di2EipN6WAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader, \n",
        "        model=model_1, \n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_1,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "35b9317aa0484ff3864208e237da2b78",
            "c455b047e523458dbadca80179469062",
            "a8843c87f0214b7e9be5313b907b909c",
            "b9607d7909234c50aca55bc8b2c730a6",
            "fb133b1166274f8baa8f0efd69009cbc",
            "9fe5730eb1f940ccb11282e9f8bf7594",
            "57e260cf47c44837a90c0c23a4ce25e4",
            "06710d032c734865aecaff27f25fb1e4",
            "06f62e6093524f0e951069bf05970ce9",
            "f2e8410a38184b41bdf071b883c3e87e",
            "4d9ce65b20c84fb98550ee410a732549"
          ]
        },
        "id": "bM7fCeC46_Vi",
        "outputId": "0b5265e9-3b62-4455-c132-58856600c696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35b9317aa0484ff3864208e237da2b78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 1.09199 | Train accuracy: 61.34%\n",
            "Test loss: 0.95636 | Test accuracy: 65.00%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.78101 | Train accuracy: 71.93%\n",
            "Test loss: 0.72227 | Test accuracy: 73.91%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.67027 | Train accuracy: 75.94%\n",
            "Test loss: 0.68500 | Test accuracy: 75.02%\n",
            "\n",
            "Train time on cpu: 25.751 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.\n",
        "\n",
        "So for smaller models and datasets, the CPU might actually be the optimal place to compute on.\n",
        "\n",
        "But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.\n",
        "\n",
        "However, this is largely dependant on the hardware you're using. With practice, you will get used to where the best place to train your models is."
      ],
      "metadata": {
        "id": "06Aj74mKKV12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Note: This will error due to `eval_model()` not using device agnostic code \n",
        "model_1_results = eval_model(model=model_1, \n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, \n",
        "    accuracy_fn=accuracy_fn) \n",
        "print(model_1_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqzKFSmpKZFk",
        "outputId": "a5b991fa-d744-477a-f2e3-61bf1bc4a1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'Model2',\n",
              " 'model_loss': 0.6850009560585022,\n",
              " 'model_acc': 75.01996805111821}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move values to device\n",
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module, \n",
        "               data_loader: torch.utils.data.DataLoader, \n",
        "               loss_fn: torch.nn.Module, \n",
        "               accuracy_fn, \n",
        "               device: torch.device = device):\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # Send data to the target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "        \n",
        "        # Scale loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 1 results with device-agnostic code \n",
        "model_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(model_0_results)\n",
        "print(model_1_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEpw0oP5KtUM",
        "outputId": "95315cf8-085b-4bcc-f402-d993277d210f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model_name': 'FirstModel', 'model_loss': 0.47663894295692444, 'model_acc': 83.42651757188499}\n",
            "{'model_name': 'Model2', 'model_loss': 0.6850009560585022, 'model_acc': 75.01996805111821}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.\n",
        "\n",
        "That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.\n",
        "\n",
        "And then the thing you thought might not work does.\n",
        "\n",
        "It's part science, part art.\n",
        "\n",
        "From the looks of things, it seems like our model is overfitting on the training data.\n",
        "\n",
        "Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.\n",
        "\n",
        "Two of the main to fix overfitting include:\n",
        "\n",
        "Using a smaller or different model (some models fit certain kinds of data better than others).\n",
        "Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns).\n",
        "There are more, but I'm going to leave that as a challenge for you to explore.\n",
        "\n",
        "Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.\n",
        "\n",
        "In the meantime, let's take a look at number 1: using a different model.\n",
        "\n",
        "\n",
        "##6. Model 2: Building a Convolutional Neural Network (CNN)\n",
        "Alright, time to step things up a notch.\n",
        "\n",
        "\n",
        "It's time to create a Convolutional Neural Network (CNN or ConvNet).\n",
        "\n",
        "CNN's are known for their capabilities to find patterns in visual data.\n",
        "\n",
        "And since we're dealing with visual data, let's see if using a CNN model can improve upon our baseline.\n",
        "\n",
        "The CNN model we're going to be using is known as TinyVGG from the CNN Explainer website.\n",
        "\n",
        "It follows the typical structure of a convolutional neural network:\n",
        "\n",
        "- Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Output layer\n",
        "\n",
        "Where the contents of [Convolutional layer -> activation layer -> pooling layer] can be upscaled and repeated multiple times, depending on requirements.\n",
        "\n",
        "<div class=\"md-typeset__scrollwrap\"><div class=\"md-typeset__table\"><table>\n",
        "<thead><tr>\n",
        "<th><strong>Problem type</strong></th>\n",
        "<th><strong>Model to use (generally)</strong></th>\n",
        "<th><strong>Code example</strong></th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td>Structured data (Excel spreadsheets, row and column data)</td>\n",
        "<td>Gradient boosted models, Random Forests, XGBoost</td>\n",
        "<td><a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\"><code>sklearn.ensemble</code></a>, <a href=\"https://xgboost.readthedocs.io/en/stable/\">XGBoost library</a></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Unstructured data (images, audio, language)</td>\n",
        "<td>Convolutional Neural Networks, Transformers</td>\n",
        "<td><a href=\"https://pytorch.org/vision/stable/models.html\"><code>torchvision.models</code></a>, <a href=\"https://huggingface.co/docs/transformers/index\">HuggingFace Transformers</a></td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table></div></div>\n",
        "\n",
        "To do so, we'll leverage the nn.Conv2d() and nn.MaxPool2d() layers from torch.nn."
      ],
      "metadata": {
        "id": "cz9b2JVnLzJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network \n",
        "class Model3(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from: \n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, \n",
        "                      out_channels=hidden_units, \n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, \n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from? \n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*7*7, \n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_2 = Model3(input_shape=1, \n",
        "    hidden_units=10, \n",
        "    output_shape=len(class_names)).to(device)\n",
        "print(model_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PNLcAKkNtJG",
        "outputId": "1e12d0f2-2aaa-4e19-b621-e032d4ffa502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model3(\n",
            "  (block_1): Sequential(\n",
            "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block_2): Sequential(\n",
            "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Stepping through nn.Conv2d()\n",
        "\n",
        "We could start using our model above and see what happens but let's first step through the two new layers we've added:\n",
        "\n",
        "nn.Conv2d(), also known as a convolutional layer.\n",
        "nn.MaxPool2d(), also known as a max pooling layer.\n",
        "Question: What does the \"2d\" in nn.Conv2d() stand for?\n",
        "\n",
        "The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's color channel dimension but each of the color channel dimensions have two dimensions too: height and width.\n",
        "\n",
        "For other dimensional data (such as 1D for text or 3D for 3D objects) there's also nn.Conv1d() and nn.Conv3d().\n",
        "\n",
        "To test the layers out, let's create some toy data just like the data used on CNN Explainer."
      ],
      "metadata": {
        "id": "2Gy7Lw13Y0CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create sample batch of random numbers with same size as image batch\n",
        "images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\n",
        "test_image = images[0] # get a single image for testing\n",
        "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\") \n",
        "print(f\"Single image pixel values:\\n{test_image}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUon5v1pZV9N",
        "outputId": "7179d5bf-3b58-40ba-e6e7-fa195538a488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
            "Single image shape: torch.Size([3, 64, 64]) -> [color_channels, height, width]\n",
            "Single image pixel values:\n",
            "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
            "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
            "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
            "         ...,\n",
            "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
            "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
            "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
            "\n",
            "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
            "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
            "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
            "         ...,\n",
            "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
            "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
            "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
            "\n",
            "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
            "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
            "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
            "         ...,\n",
            "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
            "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
            "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an example nn.Conv2d() with various parameters:\n",
        "\n",
        "- in_channels (int) - Number of channels in the input image.\n",
        "- out_channels (int) - Number of channels produced by the convolution.\n",
        "kernel_size (int or tuple) - Size of the convolving kernel/filter.\n",
        "stride (int or tuple, optional) \n",
        "- How big of a step the convolving kernel takes at a time. Default: 1.\n",
        "padding (int, tuple, str) \n",
        "- Padding added to all four sides of input. Default: 0.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv2d-layer.gif\" alt=\"example of going through the different parameters of a Conv2d layer\">\n",
        "\n",
        "If we try to pass a single image in, we get a shape mismatch error:\n",
        "\n",
        "\n",
        "\n",
        "Note: If you're running PyTorch 1.11.0+, this error won't occur.\n",
        "\n",
        "This is because our nn.Conv2d() layer expects a 4-dimensional tensor as input with size (N, C, H, W) or [batch_size, color_channels, height, width].\n",
        "\n",
        "Right now our single image test_image only has a shape of [color_channels, height, width] or [3, 64, 64].\n",
        "\n",
        "We can fix this for a single image using test_image.unsqueeze(dim=0) to add an extra dimension for N."
      ],
      "metadata": {
        "id": "-_03hyf_ZvAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a convolutional layer with same dimensions as TinyVGG \n",
        "# (try changing any of the parameters and see what happens)\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=0) # also try using \"valid\" or \"same\" here \n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_layer(test_image) # Note: If running PyTorch <1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input) "
      ],
      "metadata": {
        "id": "KEjfncMZaoxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add extra dimension to test image\n",
        "test_image.unsqueeze(dim=0).shape\n",
        "# Pass test image with extra dimension through conv_layer\n",
        "conv_layer(test_image.unsqueeze(dim=0)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APSLtO6Fagv5",
        "outputId": "980f78e5-0353-43fd-cd19-dba8d97e69f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 62, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
        "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n",
        "                         out_channels=10,\n",
        "                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n",
        "                         stride=2,\n",
        "                         padding=0)\n",
        "\n",
        "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
        "conv_layer_2(test_image.unsqueeze(dim=0)).shape\n",
        "\n",
        "# Check out the conv_layer_2 internal parameters\n",
        "print(conv_layer_2.state_dict())"
      ],
      "metadata": {
        "id": "QHMTF_Xoa5EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at that! A bunch of random numbers for a weight and bias tensor.\n",
        "\n",
        "The shapes of these are manipulated by the inputs we passed to nn.Conv2d() when we set it up.\n",
        "\n",
        "Let's check them out."
      ],
      "metadata": {
        "id": "FW7nWY1abmDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shapes of weight and bias tensors within conv_layer_2\n",
        "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
        "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels=10]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wciu82mMboGL",
        "outputId": "0630b8e5-0184-4af4-df18-4695fbfe9b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv_layer_2 weight shape: \n",
            "torch.Size([10, 3, 5, 5]) -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n",
            "\n",
            "conv_layer_2 bias shape: \n",
            "torch.Size([10]) -> [out_channels=10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.2 Stepping through nn.MaxPool2d()\n",
        "\n",
        "Now let's check out what happens when we move data through nn.MaxPool2d()."
      ],
      "metadata": {
        "id": "XPu3jZ28b52X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out original image shape without and with unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPoo2d() layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2E_4eG-cTu6",
        "outputId": "7a4f7c8b-eefa-439e-8210-73a58fdd3182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test image original shape: torch.Size([3, 64, 64])\n",
            "Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\n",
            "Shape after going through conv_layer(): torch.Size([1, 10, 62, 62])\n",
            "Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the change in the shapes of what's happening in and out of a nn.MaxPool2d() layer.\n",
        "\n",
        "The kernel_size of the nn.MaxPool2d() layer will effects the size of the output shape.\n",
        "\n",
        "In our case, the shape halves from a 62x62 image to 31x31 image."
      ],
      "metadata": {
        "id": "lPtt4_uWclZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a random tensor with a similiar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
        "print(f\"Random tensor:\\n{random_tensor}\")\n",
        "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
        "\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n",
        "\n",
        "# Pass the random tensor through the max pool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\nMax pool tensor:\\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
        "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm2ZfTBBcpgY",
        "outputId": "3c18d6f8-e756-46b1-b034-69fb376f0654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random tensor:\n",
            "tensor([[[[0.3367, 0.1288],\n",
            "          [0.2345, 0.2303]]]])\n",
            "Random tensor shape: torch.Size([1, 1, 2, 2])\n",
            "\n",
            "Max pool tensor:\n",
            "tensor([[[[0.3367]]]]) <- this is the maximum value from random_tensor\n",
            "Max pool tensor shape: torch.Size([1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the final two dimensions between random_tensor and max_pool_tensor, they go from [2, 2] to [1, 1].\n",
        "\n",
        "In essence, they get halved.\n",
        "\n",
        "And the change would be different for different values of kernel_size for nn.MaxPool2d().\n",
        "\n",
        "Also notice the value leftover in max_pool_tensor is the maximum value from random_tensor.\n",
        "\n",
        "What's happening here?\n",
        "\n",
        "This is another important piece of the puzzle of neural networks.\n",
        "\n",
        "Essentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.\n",
        "\n",
        "In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.\n",
        "\n",
        "From an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-conv-net-as-compression.png\"/>\n",
        "\n",
        "This means, that from the point of view of a neural network, intelligence is compression.\n",
        "\n",
        "This is the idea of the use of a nn.MaxPool2d() layer: take the maximum value from a portion of a tensor and disregard the rest.\n",
        "\n",
        "In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.\n",
        "\n",
        "It is the same story for a nn.Conv2d() layer.\n",
        "\n",
        "Except instead of just taking the maximum, the nn.Conv2d() performs a conovlutional operation on the data (see this in action on the CNN Explainer webpage).\n",
        "\n",
        "Exercise: What do you think the nn.AvgPool2d() layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.\n",
        "\n",
        "Extra-curriculum: Lookup \"most common convolutional neural networks\", what architectures do you find? Are any of them contained within the torchvision.models library? What do you think you could do with these?"
      ],
      "metadata": {
        "id": "7hYWJPjTdD0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.3 Setup a loss function and optimizer for model_2\n",
        "\n",
        "We've stepped through the layers in our first CNN enough.\n",
        "\n",
        "But remember, if something still isn't clear, try starting small.\n",
        "\n",
        "Pick a single layer of a model, pass some data through it and see what happens.\n",
        "\n",
        "Now it's time to move forward and get to training!\n",
        "\n",
        "Let's setup a loss function and an optimizer.\n",
        "\n",
        "We'll use the functions as before, nn.CrossEntropyLoss() as the loss function (since we're working with multi-class classification data).\n",
        "\n",
        "And torch.optim.SGD() as the optimizer to optimize model_2.parameters() with a learning rate of 0.1."
      ],
      "metadata": {
        "id": "SP_5PhKZeNy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(), \n",
        "                             lr=0.1)"
      ],
      "metadata": {
        "id": "vJCWn3dfeY2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###6.4 Training and testing model_2 using our training and test functions\n",
        "\n",
        "Loss and optimizer ready!\n",
        "\n",
        "Time to train and test.\n",
        "\n",
        "We'll use our train_step() and test_step() functions we created before.\n",
        "\n",
        "We'll also measure the time to compare it to our other models."
      ],
      "metadata": {
        "id": "HP7eh0LxelgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Train and test model \n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader, \n",
        "        model=model_2, \n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "    test_step(data_loader=test_dataloader,\n",
        "        model=model_2,\n",
        "        loss_fn=loss_fn,\n",
        "        accuracy_fn=accuracy_fn\n",
        "    )\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                           end=train_time_end_model_2,\n",
        "                                           device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "e2b8fde0b98f4e02ab23ec5ee4286c5f",
            "c0ca94c2fb984d48b4ef41911d4b78d8",
            "2eef07416af64ab5b320c9676fc8160d",
            "6858727570884bc498a527117bbe2d95",
            "a12f9980f900483e81f33fc6e97f05c7",
            "c2616a48e0354c6381f816fc3013c8ab",
            "a106e49da79d4820aa02378071a89970",
            "97db9c4322d94386b593ee1a2db6a4f9",
            "d3c0020d171045d9925538e0d8893e29",
            "ab6aa3ecd80747dcba9873f6f82f9c4e",
            "6ebd68fe53d740959ec02bc0f0cb9266"
          ]
        },
        "id": "XENvxu4Des7X",
        "outputId": "3c09d626-ee02-41b3-f007-35661ffaeeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2b8fde0b98f4e02ab23ec5ee4286c5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "---------\n",
            "Train loss: 0.36184 | Train accuracy: 86.87%\n",
            "Test loss: 0.35975 | Test accuracy: 86.75%\n",
            "\n",
            "Epoch: 1\n",
            "---------\n",
            "Train loss: 0.32664 | Train accuracy: 88.12%\n",
            "Test loss: 0.33187 | Test accuracy: 87.73%\n",
            "\n",
            "Epoch: 2\n",
            "---------\n",
            "Train loss: 0.30604 | Train accuracy: 88.92%\n",
            "Test loss: 0.31678 | Test accuracy: 88.45%\n",
            "\n",
            "Train time on cpu: 171.037 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results \n",
        "model_2_results = eval_model(\n",
        "    model=model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS2ZdYt9gYwf",
        "outputId": "cbf02218-6a07-4e06-8a62-8d7e00915da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'Model3',\n",
              " 'model_loss': 0.3167797029018402,\n",
              " 'model_acc': 88.44848242811501}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Compare model results and training time\n",
        "We've trained three different models.\n",
        "\n",
        "- model_0 - our baseline model with two nn.Linear() layers.\n",
        "model_1 - the same setup as our baseline model except with nn.ReLU() layers in between the nn.Linear() layers.\n",
        "- model_2 - our first CNN model that mimics the TinyVGG architecture on the CNN Explainer website.\n",
        "\n",
        "This is a regular practice in machine learning.\n",
        "\n",
        "Building multiple models and performing multiple training experiments to see which performs best.\n",
        "\n",
        "Let's combine our model results dictionaries into a DataFrame and find out."
      ],
      "metadata": {
        "id": "eiK4pUlfgjyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "O_PljVfTgtPz",
        "outputId": "335b8928-0b63-479b-853f-570df775b4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   model_name  model_loss  model_acc\n",
              "0  FirstModel    0.476639  83.426518\n",
              "1      Model2    0.685001  75.019968\n",
              "2      Model3    0.316780  88.448482"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e61d9542-62df-43bb-b2ee-9024e6d0fb14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_loss</th>\n",
              "      <th>model_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FirstModel</td>\n",
              "      <td>0.476639</td>\n",
              "      <td>83.426518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Model2</td>\n",
              "      <td>0.685001</td>\n",
              "      <td>75.019968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Model3</td>\n",
              "      <td>0.316780</td>\n",
              "      <td>88.448482</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e61d9542-62df-43bb-b2ee-9024e6d0fb14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e61d9542-62df-43bb-b2ee-9024e6d0fb14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e61d9542-62df-43bb-b2ee-9024e6d0fb14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}