{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUJNeYpBDO9f+e4rY4XQP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan95614/Pytorch-Lessons/blob/RUNTHISFILE/Pytorch_Tutorial_1_Fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKQ8SMDMAxms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7257d5e9-b3f1-40c1-abb1-5d1da076a2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# Making sure that Pytorch is here Installed\n",
        "import torch \n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are the fundamental building block of machine learning.\n",
        "\n",
        "Their job is to represent data in a numerical way.\n",
        "\n",
        "For example, you could represent an image as a tensor with shape [3, 224, 224] which would mean [colour_channels, height, width], as in the image has 3 colour channels (red, green, blue), a height of 224 pixels and a width of 224 pixels.\n",
        "\n",
        "You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside ([) and you only need to count one side.\n"
      ],
      "metadata": {
        "id": "4YaOaWCIFh-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Scalar -> Zero Dimension Tensor\n",
        "Scaler = torch.tensor(7)\n",
        "print(Scaler.ndim) # 0 because it shows the dimensions as we said\n",
        "print(Scaler.item())"
      ],
      "metadata": {
        "id": "Gq7kcqxPFjPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vector is a single dimension tensor but can contain many numbers.\n",
        "\n",
        "As in, you could have a vector [3, 2] to describe [bedrooms, bathrooms] in your house. Or you could have [3, 2, 2] to describe [bedrooms, bathrooms, car_parks] in your house.\n",
        "\n",
        "The important trend here is that a vector is flexible in what it can represent (the same with tensors)."
      ],
      "metadata": {
        "id": "vAxh0L7AG5Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Vector -> Flexible it what it can be\n",
        "Vector = torch.tensor([7, 7])\n",
        "print(Vector.shape) #torch.Size([2]): tells you how it is shaped like that"
      ],
      "metadata": {
        "id": "B4pB-UH9G6DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A matrix is a bit different but will have one more dimension compared to a vector, it also has the same characterisitics.\n"
      ],
      "metadata": {
        "id": "lp-3JC_ZIvlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix\n",
        "MATRIX = torch.tensor([[7, 8], \n",
        "                       [9, 10]])\n",
        "print(MATRIX.ndim)  # 2: We can tell that this already 2 dimensional and unlike a vector can carry more paramaters\n",
        "print(MATRIX.shape) # torch.Size([2, 2]): We can see two subarrays with two elements, making it 2-2"
      ],
      "metadata": {
        "id": "4Bl7Y9wUI3lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor\n",
        "Tensor = torch.tensor([[[1, 2, 3],\n",
        "                        [3, 6, 9],\n",
        "                        [2, 4, 5]]])\n",
        "print(Tensor.ndim) # 3 This is 3 dimensional \n",
        "print(Tensor.shape)# torch.Size([1, 3, 3]) Do it in reverse and you will be able to tell"
      ],
      "metadata": {
        "id": "SM5Vd7j-ROrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've established tensors represent some form of data.\n",
        "\n",
        "And machine learning models such as neural networks manipulate and seek patterns within tensors.\n",
        "\n",
        "But when building machine learning models with PyTorch, it's rare you'll create tenors by hand (like what we've being doing).\n",
        "\n",
        "Instead, a machine learning model often starts out with large random tensors of numbers and adjusts these random numbers as it works through data to better represent it.\n",
        "\n",
        "In essence:\n",
        "\n",
        "Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers...\n",
        "\n",
        "As a data scientist, you can define how the machine learning model starts (initialization), looks at data (representation) and updates (optimization) its random numbers."
      ],
      "metadata": {
        "id": "j55iwFMtp0Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can generate random tensors that already have their data fitted into them \n",
        "\n",
        "# Create a random tensor of size (3, 4)\n",
        "random_tensor = torch.rand(size=(3, 4))\n",
        "print(random_tensor)\n",
        "print(random_tensor.dtype)\n",
        "\n",
        "# Making sure that only 0s and 1s are filled in it\n",
        "zeros = torch.zeros(size=(3, 4))\n",
        "print(zeros)\n",
        "print(zeros.dtype)\n",
        "\n",
        "ones = torch.ones(size=(3, 4))\n",
        "print(ones)\n",
        "print(ones.dtype , \"\\n\")\n",
        "\n",
        "# Creating any sort of ranges and creating a range of values 0 to 10\n",
        "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
        "print(zero_to_ten)\n",
        "\n",
        "# Creating a comparitive tensor of zeros\n",
        "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
        "print(ten_zeros)"
      ],
      "metadata": {
        "id": "TXpgBIyHp1Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tensor datatypes\n",
        "There are many different tensor datatypes available in PyTorch.\n",
        "[link](https://pytorch.org/docs/stable/tensors.html#data-types) \n",
        "Some are specific for CPU and some are better for GPU.\n",
        "\n",
        "Getting to know which is which can take some time.\n",
        "\n",
        "Generally if you see torch.cuda anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).\n",
        "\n",
        "The most common type (and generally the default) is torch.float32 or torch.float.\n",
        "\n",
        "This is referred to as \"32-bit floating point\".\n",
        "\n",
        "But there's also 16-bit floating point (torch.float16 or torch.half) and 64-bit floating point (torch.float64 or torch.double).\n"
      ],
      "metadata": {
        "id": "p_cdUSmlmowQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default datatype for tensors is float32\n",
        "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
        "                               device=None, # defaults to None, which uses the default tensor type\n",
        "                               requires_grad=False) # if True, operations perfromed on the tensor are recorded \n",
        "\n",
        "print(float_32_tensor.shape)\n",
        "print(float_32_tensor.dtype)\n",
        "print(float_32_tensor.device) # cpu written(PyTorch likes calculations between tensors to be on the same device).\n",
        "\n",
        "\n",
        "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=torch.float16) # torch.half would also work\n",
        "print(float_16_tensor.dtype)"
      ],
      "metadata": {
        "id": "8MAeoW5xmpJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting information from tensors\n",
        "\n",
        "Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.\n",
        "\n",
        "We've seen these before but three of the most common attributes you'll want to find out about tensors are:\n",
        "\n",
        "shape - what shape is the tensor? (some operations require specific shape rules)\n",
        "dtype - what datatype are the elements within the tensor stored in?\n",
        "device - what device is the tensor stored on? (usually GPU or CPU)\n",
        "Let's create a random tensor and find out details about it."
      ],
      "metadata": {
        "id": "24ssEaPEoGkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "some_tensor = torch.rand(3, 4)\n",
        "\n",
        "# Find out details about it\n",
        "print(some_tensor)\n",
        "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n",
        "\n",
        "\n",
        "#Note: When you run into issues in PyTorch, it's very often\n",
        "#one to do with one of the three attributes above. \n",
        "#So when the error messages show up, sing yourself a \n",
        "#little song called \"what, what, where\":\n",
        "\n",
        "#\"what shape are my tensors? what datatype are they and where are they stored? \n",
        "#what shape, what datatype, where where where\"\n",
        "\n"
      ],
      "metadata": {
        "id": "H1AifcXKoKr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manipulating tensors (tensor operations)\n",
        "In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\n",
        "\n",
        "A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\n",
        "\n",
        "These operations are often a wonderful dance between:\n",
        "\n",
        "- Addition\n",
        "- Substraction\n",
        "- Multiplication (element-wise)\n",
        "- Division\n",
        "- Matrix multiplication\n",
        "And that's it. Sure there are a few more here and there but these are the basic building blocks of neural networks.\n",
        "\n",
        "Stacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!)."
      ],
      "metadata": {
        "id": "M3pe5o2UoXyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor of values and add a number to it\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "print(tensor + 10) # make sure to assign variables to this to better it\n",
        "\n",
        "# Subtract and reassign\n",
        "tensor = tensor - 10\n",
        "print(tensor) # tensor([-9, -8, -7])\n",
        "\n",
        "# Add and reassign\n",
        "tensor = tensor + 10\n",
        "print(tensor) #tensor([1, 2, 3]) -> remember we reassigned this"
      ],
      "metadata": {
        "id": "gI4ghDWJpE74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "PyTorch also has a bunch of built-in functions like torch.mul() (short for multiplcation) and torch.add() to perform basic operations."
      ],
      "metadata": {
        "id": "_cN1kOziWNM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also use torch functions\n",
        "tensor = torch.multiply(torch.tensor([1, 2, 3]), 10)\n",
        "\n",
        "# Original tensor is still unchanged \n",
        "print(tensor) # tensor([10, 20, 30]) -> used some basic encapsulation "
      ],
      "metadata": {
        "id": "3GLxh01MWPWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Matrix Multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
        "\n",
        "One of the most common operations in machine learning and deep learning algorithms (like neural networks) is matrix multiplication.\n",
        "\n",
        "PyTorch implements matrix multiplication functionality in the torch.matmul() method.\n",
        "\n",
        "The main two rules for matrix multiplication to remember are:\n",
        "\n",
        "The inner dimensions must match:\n",
        "**Notice how they have to be different sizes as one row's are multiplied by the other's columns**\n",
        "- (3, 2) @ (3, 2) won't work\n",
        "- (2, 3) @ (3, 2) will work\n",
        "- (3, 2) @ (2, 3) will work\n",
        "\n",
        "The resulting matrix has the shape of the outer dimensions:\n",
        "\n",
        "- (2, 3) @ (3, 2) -> (2, 2)\n",
        "- (3, 2) @ (2, 3) -> (3, 3)"
      ],
      "metadata": {
        "id": "qcvv9LoUW4AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%time\n",
        "'''\n",
        "CPU times: user 4.36 ms, sys: 0 ns, total: 4.36 ms\n",
        "Wall time: 7.08 ms\n",
        "'''\n",
        "# Just intializing \n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "print(tensor.shape)\n",
        "\n",
        "# Element-wise matrix multiplication\n",
        "print(tensor * tensor) # tensor([1, 4, 9])\n",
        "\n",
        "\n",
        "# Matrix multiplication: Method 1\n",
        "print(torch.matmul(tensor, tensor)) # tensor(14)\n",
        "\n",
        "# Matrix multiplication: Method 2\n",
        "print(tensor @ tensor) # tensor(14) -> extremely discouraged \n"
      ],
      "metadata": {
        "id": "8MJdllF6XzAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One of the most common errors in deep learning (shape errors)\n",
        "\n",
        "Because much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches."
      ],
      "metadata": {
        "id": "Gy_wHeCbaguI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes need to be in the right way  \n",
        "tensor_A = torch.tensor([[1, 2],\n",
        "                         [3, 4],\n",
        "                         [5, 6]], dtype=torch.float32)\n",
        "\n",
        "tensor_B = torch.tensor([[7, 10],\n",
        "                         [8, 11], \n",
        "                         [9, 12]], dtype=torch.float32)\n",
        "\n",
        "try:\n",
        "  torch.matmul(tensor_A, tensor_B) # (this will error)\n",
        "except RuntimeError as e:\n",
        "  print(\" Extremely common mistake it is, you have to remember to look at the bolded text in the above above column\") # Yoda Voice"
      ],
      "metadata": {
        "id": "ZXIzQDLtahMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7659039b-5820-4595-a9b3-cf8724752c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Extremely common mistake it is, you have to remember to look at the bolded text in the above above column\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##One of the ways to do this is with a transpose (switch the dimensions of a given tensor).\n",
        "\n",
        "You can perform transposes in PyTorch using either:\n",
        "\n",
        "torch.transpose(input, dim0, dim1) \n",
        "- where input is the desired tensor to transpose and dim0 and dim1 are the dimensions to be swapped.\n",
        "tensor.T \n",
        "- where tensor is the desired tensor to transpose.\n",
        "\n",
        "Let's try the **latter**."
      ],
      "metadata": {
        "id": "A2PWCCXxbIG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View tensor_A and tensor_B\n",
        "print(tensor_A)\n",
        "print(tensor_B)\n",
        "print()\n",
        "\n",
        "#Fixing the issue, lets view tensor_A and tensor_B.T\n",
        "print(tensor_A)\n",
        "print(tensor_B.T)\n",
        "\n",
        "# Visit http://matrixmultiplication.xyz/. \n",
        "\n",
        "# torch.mm is a shortcut for matmul\n",
        "print(\"This is a shortcut to it\\n\" , torch.mm(tensor_A, tensor_B.T))"
      ],
      "metadata": {
        "id": "hmqVF7wxbQKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are full of matrix multiplications and dot products.\n",
        "\n",
        "The torch.nn.Linear() module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input x and a weights matrix A.\n",
        "\n",
        "y\n",
        "=\n",
        "x\n",
        "⋅\n",
        "A\n",
        "ᵀ\n",
        "+\n",
        "b\n",
        "\n",
        "Where:\n",
        "\n",
        "- x is the input to the layer (deep learning is a stack of layers like torch.nn.Linear() and others on top of each other).\n",
        "- A is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"T\", that's because the weights matrix gets transposed).\n",
        "- Note: **You might also often see W or another letter like X used to showcase the weights matrix.**\n",
        "- b is the bias term used to slightly offset the weights and inputs.\n",
        "y is the output (a manipulation of the input in the hopes to discover patterns in it).\n",
        "\n",
        "This is a linear function (you may have seen something like \n",
        "y=mx+b in high school or elsewhere), and can be used to draw a straight line!"
      ],
      "metadata": {
        "id": "9np3LkYjc2w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# This uses matrix multiplication\n",
        "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n",
        "                         out_features=6) # out_features = describes outer value amount\n",
        "x = tensor_A\n",
        "print(tensor_A)\n",
        "output = linear(x)\n",
        "print(f\"Input shape: {x.shape}\\n\")\n",
        "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
      ],
      "metadata": {
        "id": "MVkx5EmUdDnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Finding the min, max, mean, sum, etc (aggregation)\n",
        "\n",
        "Now we've seen a few ways to manipulate tensors, let's run through a few ways to aggregate them (go from more values to less values).\n",
        "\n",
        "First we'll create a tensor and then find the max, min, mean and sum of it."
      ],
      "metadata": {
        "id": "QL01pfJweqIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "x = torch.arange(0, 100, 10) # start, end, step\n",
        "print(x)\n",
        "\n",
        "# Built in methods for the object itself\n",
        "print(f\"Minimum: {x.min()}\")\n",
        "print(f\"Maximum: {x.max()}\")\n",
        "# print(f\"Mean: {x.mean()}\") # this will error\n",
        "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n",
        "print(f\"Sum: {x.sum()}\")\n",
        "\n",
        "#Built in functions to go to the object\n",
        "print(torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x))"
      ],
      "metadata": {
        "id": "fXLDUkP6foL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Positional min/max\n",
        "\n",
        "You can also find the index of a tensor where the max or minimum occurs with torch.argmax() and torch.argmin() respectively.\n",
        "\n",
        "This is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (we'll see this in a later section when using the softmax activation function)."
      ],
      "metadata": {
        "id": "I2ZybvVfgVRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "tensor = torch.arange(10, 100, 10)\n",
        "print(f\"Tensor: {tensor}\")\n",
        "\n",
        "# Returns index of max and min values\n",
        "print(f\"Index where max value occurs: {tensor.argmax()}\") # Will be used in the softmax activation layer which is pretty cool\n",
        "print(f\"Index where min value occurs: {tensor.argmin()}\\n\\n\") # Will be used in the softmax activation layer which is pretty cool\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "The softmax function is often used as the last activation function of a neural\n",
        "network to normalize the output of a network to a probability distribution \n",
        "over predicted output classes, based on Luce's choice axiom.\n",
        "\n",
        "\n",
        "That is, softmax is used as the activation function for multi-class \n",
        "classification problems where class membership is required on more than two class labels.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cnL1ZJVNgbVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Different datatypes can be confusing to begin with. But think of it like this, the lower the number (e.g. 32, 16, 8), the less precise a computer stores the value. And with a lower amount of storage, this generally results in faster computation and a smaller overall model. Mobile-based neural networks often operate with 8-bit integers, smaller and faster to run but less accurate than their float32 counterparts. For more on this, I'd read up about [precision in computing](https://www.wikiwand.com/en/Precision_(computer_science)).\n",
        "\n",
        "\n",
        "Exercise: So far we've covered a fair few tensor methods but there's a bunch more in the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html), I'd recommend spending 10-minutes scrolling through and looking into any that catch your eye. Click on them and then write them out in code yourself to see what happens."
      ],
      "metadata": {
        "id": "3-VYGknZhaCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make the right elements of your tensors are mixing with the right elements of other tensors.\n",
        "\n",
        "Let's try them out."
      ],
      "metadata": {
        "id": "GK83TWm-ij0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "import torch\n",
        "x = torch.arange(1., 8.)\n",
        "print(x)\n",
        "print(x.shape)\n",
        "\n",
        "# Add an extra dimension\n",
        "x_reshaped = x.reshape(7, 1)\n",
        "print(x_reshaped)\n",
        "print(x_reshaped.shape)\n",
        "\n",
        "# This code actually changes the code of the\n",
        "# object itself and reformats it with the view\n",
        "z = x.view(1, 7)\n",
        "print(z)\n",
        "print(x)\n",
        "\n",
        "# Stack tensors on top of each other\n",
        "x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n",
        "print(\"\\n\", x_stacked) # this can actually just stack tensors\n",
        "\n",
        "\n",
        "print(f\"Previous tensor: {x_reshaped}\")\n",
        "print(f\"Previous shape: {x_reshaped.shape}\")\n",
        "\n",
        "# Remove extra dimension from x_reshaped\n",
        "x_squeezed = x_reshaped.squeeze()\n",
        "print(f\"\\nNew tensor: {x_squeezed}\")\n",
        "print(f\"New shape: {x_squeezed.shape}\")\n",
        "\n",
        "\n",
        "# Create tensor with specific shape\n",
        "x_original = torch.rand(size=(224, 224, 3))\n",
        "\n",
        "# Permute the original tensor to rearrange the axis order\n",
        "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
        "\n",
        "print(f\"Previous shape: {x_original.shape}\")\n",
        "print(f\"New shape: {x_permuted.shape}\")\n",
        "\n",
        "'''Note: Because permuting returns a view \n",
        "(shares the same data as the original), \n",
        "the values in the permuted tensor will be\n",
        " the same as the original tensor and if you \n",
        " change the values in the view, it will change\n",
        "  the values of the original.'''"
      ],
      "metadata": {
        "id": "ZXy51BtQik3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Indexing (selecting data from tensors)\n",
        "Sometimes you'll want to select specific data from tensors (for example, only the first column or second row).\n",
        "\n",
        "To do so, you can use indexing.\n",
        "\n",
        "If you've ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar."
      ],
      "metadata": {
        "id": "RKMLNEe4nKYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor \n",
        "import torch\n",
        "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
        "print(x)\n",
        "print(x.shape) # torch.Size([1, 3, 3])\n",
        "\n",
        "# Let's index bracket by bracket\n",
        "print(f\"First square bracket:\\n{x[0]}\") \n",
        "print(f\"Second square bracket: {x[0][0]}\")  #tensor([1, 2, 3])\n",
        "print(f\"Third square bracket: {x[0][0][0]}\")# 1\n",
        "print()\n",
        "\n",
        "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
        "print(x[:, 0])\n",
        "\n",
        "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
        "print(x[:, :, 1])\n",
        "\n",
        "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
        "print(x[:, 1, 1])\n",
        "\n",
        "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
        "print(x[0, 0, :]) # same as x[0][0]"
      ],
      "metadata": {
        "id": "HVGP29_znfCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PyTorch tensors & NumPy\n",
        "\n",
        "Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.\n",
        "\n",
        "The two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n",
        "\n",
        "- torch.from_numpy(ndarray) - NumPy array -> PyTorch tensor.\n",
        "- torch.Tensor.numpy() - PyTorch tensor -> NumPy array.\n",
        "\n",
        "Let's try them out.\n",
        "\n",
        "Note: By default, NumPy arrays are created with the datatype float64 and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).\n",
        "\n",
        "However, many PyTorch calculations default to using float32.\n",
        "\n",
        "So if you want to convert your NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32), you can use:\n",
        "\n",
        "**tensor = torch.from_numpy(array).type(torch.float32).**"
      ],
      "metadata": {
        "id": "LfNkDlUqp0A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy array to tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "array = np.arange(1.0, 8.0)\n",
        "tensor = torch.from_numpy(array).type(torch.float32)\n",
        "print(array)\n",
        "print(tensor)\n",
        "print()\n",
        "\n",
        "# Tensor to NumPy array\n",
        "tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n",
        "numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n",
        "print(tensor)\n",
        "print(numpy_tensor)"
      ],
      "metadata": {
        "id": "NA8-M6sPqPU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Reproducibility (trying to take the random out of random)\n",
        "As you learn more about neural networks and machine learning, you'll start to discover how much randomness plays a part.\n",
        "\n",
        "Well, pseudorandomness that is. Because after all, as they're designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since I'm not a computer scientist, I'll let you find out more yourself).\n",
        "\n",
        "How does this relate to neural networks and deep learning then?\n",
        "\n",
        "We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.\n",
        "\n",
        "In short:\n",
        "```\n",
        "start with random numbers -> tensor operations -> try to make better (again and again and again)```\n",
        "\n",
        "Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.Why?So you can perform repeatable experiments.For example, you create an algorithm capable of achieving X performance.And then your friend tries it out to verify you're not crazy.How could they do such a thing?That's where reproducibility comes in.In other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?Let's see a brief example of reproducibility in PyTorch. We'll start by creating two random tensors, since they're random, you'd expect them to be different right?"
      ],
      "metadata": {
        "id": "-4H9BERBrCxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create two random tensors\n",
        "random_tensor_A = torch.rand(3, 4)\n",
        "random_tensor_B = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
        "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
        "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
        "random_tensor_A == random_tensor_B\n",
        "\n",
        "#torch.clone(random_tensor_A) use this to clone\n",
        "\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# # Set the random seed\n",
        "RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n",
        "torch.manual_seed(seed=RANDOM_SEED) \n",
        "random_tensor_C = torch.rand(3, 4)\n",
        "\n",
        "# Have to reset the seed every time a new rand() is called \n",
        "# Without this, tensor_D would be different to tensor_C \n",
        "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n",
        "random_tensor_D = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
        "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
        "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
        "random_tensor_C == random_tensor_D"
      ],
      "metadata": {
        "id": "UtgsBfv0rXww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}